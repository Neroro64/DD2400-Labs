{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import plotly.express as ex\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "__author__=\"Nuo Chen\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Model \n",
    "---\n",
    "A basic MLP network:\n",
    "\n",
    "**Input**:\n",
    "\n",
    "**Learning rule**:\n",
    "- Delta rule\n",
    "- Backpropagation (generalised delta rule)\n",
    "\n",
    "**Output**:\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, nodes):\n",
    "        \"\"\"\n",
    "        Arg:\n",
    "            layers - [in, n1, n2, ..., nj, out] an array of numbers of nodes in each layer.\n",
    "        Returns:\n",
    "            a fully-connected network with default weights and biases sampled from N(0, 1)\n",
    "        \"\"\"\n",
    "        self.n_layers = len(nodes)\n",
    "        self.NODES = nodes\n",
    "        self.init_param()\n",
    "    def init_param(self):\n",
    "        \"\"\"\n",
    "        Initialize the weights and biases with samples from normal distributions\n",
    "        W = [shape(n1,n2), shape(n2,n3), ..., shape(nj,nj+1)], where nj = number of nodes in the layer\n",
    "        B = [shape(n1,1), shape(n2,1), ..., shape(nj,1)]\n",
    "        \"\"\"\n",
    "        self.W = [np.random.normal(0,1,(i,j)) for i,j in zip(self.NODES[:-1], self.NODES[1:])]\n",
    "        self.B = [np.random.normal(0,1,(i,1)) for i in self.NODES[1:]]\n",
    "    def init_training_param(self, n, batch_size, epochs, eta, lmbda):\n",
    "        \"\"\"\n",
    "        Initialize the hyper-parameters\n",
    "        \"\"\"\n",
    "        self.n_samples = n\n",
    "        self.BATCH_SIZE = batch_size\n",
    "        self.EPOCS = epochs\n",
    "        self.BATCHES = int(n/batch_size)\n",
    "        self.LAMBDA = lmbda\n",
    "        self.ETA = eta\n",
    "    def normalize(self, x):\n",
    "        \"\"\"\n",
    "        Normalizes the inputs\n",
    "        \"\"\"\n",
    "        mean = np.mean(x, axis=1, keepdims=True)\n",
    "        std = np.std(x, axis=1, keepdims=True)\n",
    "        x = (x-mean) / std\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax():\n",
    "        return lambda x: np.exp(x-np.max(x,axis=0)) / np.sum(np.exp(x-np.max(x,axis=0)), axis=0)\n",
    "    @staticmethod\n",
    "    def relu():\n",
    "        return lambda x: np.maximum(0,x)\n",
    "    @staticmethod\n",
    "    def delta_rule():\n",
    "        return lambda x,w,t,eta: -eta*(w@x - t)@x.T    \n",
    "    @staticmethod\n",
    "    def sigmoid():\n",
    "        return lambda x: 2 / (1+np.exp(-x)) - 1\n",
    "    \n",
    "    def cross_entropy(self, p, y):\n",
    "        \"\"\"\n",
    "        Returns the cross-entropy cost of the prediciton\n",
    "        \"\"\"\n",
    "        p[p==0] = 1e-7\n",
    "        cost = 1/y.shape[1] * -np.sum(y*np.log(p))\n",
    "        w_sum = [w**2 for w in self.W]\n",
    "        s = 0\n",
    "        for w in w_sum: \n",
    "            s+= np.sum(w)\n",
    "        cost += self.LAMBDA * s\n",
    "        return cost\n",
    "    def feedforward(self, activations, act_fn, out_fn):\n",
    "        \"\"\"\n",
    "        s1 = w1 @ x + b1\n",
    "        h1 = act_fn(s1)\n",
    "        ....\n",
    "        sn = wn @ h(n-1) + bn\n",
    "        return out_fn(sn)\n",
    "        \"\"\"\n",
    "        a = activations[0]\n",
    "        for i in range(self.n_layers-1):\n",
    "            s = self.W[i].T @ a + self.B[i]\n",
    "            a = act_fn(a)\n",
    "            activations.append(a)\n",
    "\n",
    "        return out_fn(a)\n",
    "    def backPropagation(self, y, p, activations):\n",
    "        \"\"\"\n",
    "        Back propagate the network and calculate the gradients\n",
    "        \"\"\"\n",
    "        dw = [np.zeros(w.shape) for w in self.W]\n",
    "        db = [np.zeros(b.shape) for b in self.B]\n",
    "\n",
    "        g = -(y - p)\n",
    "        for i in range(len(self.W)-1, -1, -1):\n",
    "            dw[i] = g @ activations[i].T * 1/self.BATCH_SIZE + 2 * self.LAMBDA * self.W[i].T\n",
    "            db[i] =  (np.sum(g, axis=1) * 1/self.BATCH_SIZE).reshape(self.B[i].shape)\n",
    "            g = self.W[i] @ g\n",
    "            g[np.where(activations[i]<=0)] = 0\n",
    "        \n",
    "        return (dw, db)\n",
    "    def backPass(self, labels, predictions, activations):\n",
    "        dw = [np.zeros(w.shape) for w in self.W]\n",
    "        db = [np.zeros(b.shape) for b in self.B]\n",
    "\n",
    "        g = (p-y)\n",
    "        for i in range(len(self.W)-1, -1, -1):\n",
    "            def\n",
    "\n",
    "    def accuracy(self, p, y):\n",
    "        \"\"\"\n",
    "        Compute the accuracy of the predictions\n",
    "        \"\"\"\n",
    "        predictions = np.argmax(p, axis=0)\n",
    "        y = np.argmax(y, axis=0)\n",
    "        acc = predictions.T[predictions == y].shape[0] / p.shape[1]\n",
    "        return acc\n",
    "    \n",
    "    def update_batch(self, x, y):\n",
    "        \"\"\"\n",
    "        For each batch: \n",
    "            Pass the input into the network and compute the predictions.\n",
    "            Back propagate through the network to compute the gradients using the stored act>\n",
    "            Update the weights and biases using the gradients\n",
    "        \"\"\"\n",
    "        activations = [x]\n",
    "        p = self.feedforward(activations, relu(), softmax())\n",
    "        dw, db = self.backPropagation(y, p, activations)\n",
    "\n",
    "        for i in range(self.L-1):\n",
    "            self.W[i] = self.W[i] - self.ETA * dw[i].T\n",
    "            self.B[i] = self.B[i] - self.ETA * db[i]\n",
    "\n",
    "\n",
    "    def SGD(self, data, features, targets, test_size, verbose=False):\n",
    "        \"\"\"\n",
    "        Stochastic gradient descend method\n",
    "        Trains the network a given number of epochs or cycles\n",
    "        Return:\n",
    "            Training cost and validation cost\n",
    "            Training accuracy and validation accuracy\n",
    "        \"\"\"\n",
    "\n",
    "        train_features, test_features, train_labels, test_labels = train_test_split(features, targets, test_size = test_size, random_state = 2020)\n",
    "\n",
    "        training_cost = []\n",
    "        validation_cost = []\n",
    "        training_accuracy = []\n",
    "        for t in range(self.EPOCHS):\n",
    "            # Shuffles the order of samples \n",
    "            idx = np.random.permutation(self.n_samples)\n",
    "            for j in range(1, self.BATCHES):\n",
    "                start = (j-1) * self.BATCH_SIZE\n",
    "                end = j * self.BATCH_SIZE\n",
    "                indices = idx[start:end]\n",
    "                x_batch = train_features[:, indices]            \n",
    "                y_batch = train_labels[:, indices]    \n",
    "                self.update_batch(x_batch, y_batch)\n",
    "\n",
    "                    # Check cost and accuracy 10 times per cycle \n",
    "\n",
    "            p_t = self.feedforward([train_features])\n",
    "            p_v = self.feedforward([test_features])\n",
    "            training_cost.append(self.cross_entropy(p_t, train_labels))\n",
    "            validation_cost.append(self.cross_entropy(p_v, test_labels))\n",
    "            training_accuracy.append(self.accuracy(p_t, train_labels))\n",
    "            validation_accuracy.append(self.accuracy(p_v, test_labels))\n",
    "\n",
    "            if (verbose):\n",
    "                print(\"Epoch #{}--------------------------------------\".format(i))\n",
    "                print(\"Training Cost: {:.6f}\".format(training_cost[-1]))\n",
    "                print(\"Validation Cost: {:.6f}\".format(validation_cost[-1]))\n",
    "                print(\"Training Accuracy = {:.3f}\".format(training_accuracy[-1]))\n",
    "                print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy[-1]))\n",
    "                print(\"-\"*50)\n",
    "\n",
    "        return (training_cost, validation_cost, training_accuracy, validation_accuracy)\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"\n",
    "        Save the model to the file 'filename`.\n",
    "        \"\"\"\n",
    "        data = {\"Nodes\": self.NODES,\n",
    "                \"W\": [w.tolist() for w in self.W],\n",
    "                \"B\": [b.tolist() for b in self.B]}\n",
    "        with open(filename, \"w\") as f:\n",
    "            json.dump(data, f)\n",
    "\n",
    "    def load(self, filename):\n",
    "        \"\"\"\n",
    "        Load the model\n",
    "        \"\"\"\n",
    "        with open(filename, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.NODES = data[\"Nodes\"]\n",
    "        self.W = [np.array(w) for w in data[\"W\"]]\n",
    "        self.B = [np.array(b) for b in data[\"B\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}